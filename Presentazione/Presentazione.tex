\documentclass{beamer}

\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage{beamerthemeAntibes}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[utf8]{inputenc} 
\usepackage{epsfig}  
\usepackage{amsmath} 
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{changepage}

\lstset{
  basicstyle=\footnotesize,
  language=C,                % choose the language of the code                 % where to put the line-numbers
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=true,         % sets if automatic breaks should only happen at whitespace
  title=\lstname,                 % show the filename of files included with \lstinputlisting;
}



\setbeamertemplate{itemize/enumerate body begin}{\footnotesize}

\title{Social Networks}
\author{Luigi Giugliano$^1$, Steven Rosario Sirchia$^1$}
\institute{$^1$Università degli studi di Salerno}


\begin{document}

\begin{frame}
   \maketitle
\end{frame}

\begin{frame}
\frametitle{Introduction}
The purpose of our work is to test different algorithms in the three fundamental areas for assembling any search engine offering a Sponsored Search system:
\begin{itemize}
\item \textbf{Ranking} of web documents
\item \textbf{Matching} of words inside documents
\item \textbf{Auctions} for acquiring advertisement slots.
\end{itemize}
We will briefly talk about the proposed algorithms, and then compare running times and results obtained from their execution more in detail, suggesting what combination of algorithms seems to be the best for realizing a new search engine.
\end{frame}


%\begin{frame}
%  \frametitle{Overview}
%  \footnotesize \tableofcontents
%\end{frame}

\AtBeginSection[]
  {
     \begin{frame}<beamer>
     \frametitle{Overview}
   \footnotesize \tableofcontents[currentsection]
     \end{frame}
}

\section{Ranking}
\subsection{Page Rank}
\begin{frame}
\frametitle{Page Rank}
\begin{block}{Page Rank}
The intuition behind \emph{Page Rank} is:
\center``a page is important if it is cited by other important pages''.
\end{block}
This intuition rises from the usual endorsement mode, for example, among academic or governmental pages, among bloggers, or among personal pages more generally. It is also the dominant mode in the scientific literature.
\end{frame}

\begin{frame}
\frametitle{Algorithm}
We can think of PageRank as a kind of “fluid” that circulates through the network, passing from node to node across edges, and pooling at the nodes that are the most important.
\center
\includegraphics[scale=0.3]{img/general_network.png} 
\end{frame}


\begin{frame}
\frametitle{Algorithm Steps}
\begin{itemize}
\item \onslide<1-> In a network with n nodes, we assign all nodes the same initial PageRank, set to be 1/n.
\item \onslide<2-> We choose a number of steps k.
\item \onslide<3-> We then perform a sequence of k updates to the PageRank values, using the following
rule for each update:\\
\begin{adjustwidth}{2.5em}{0pt}
\textbf{Basic PageRank Update Rule}: Each page divides its current PageRank equally across its out-going links, and passes these equal shares to the pages it points to. (If a page has no out-going links, it passes all its current PageRank to itself.) Each page updates its new PageRank to be the sum of the shares it receives.
\end{adjustwidth}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The ``Wrong'' nodes}
There is a difficulty with the basic definition of PageRank, however: in many networks, the “wrong” nodes can end up with all the PageRank.
\begin{center} 
\includegraphics[scale=0.2]{img/wrong_nodes.png} 
\end{center}
The Wrong nodes are a small sets of nodes that can be reached from the rest of the graph, but have no paths back.
\end{frame}

\begin{frame}
\frametitle{Scaled PageRank }
We can use the mechanism of fluid presented before: there is a \alert{counter-balancing process} preventing that all the water stands only on downhill places on the earth.
\begin{block}{Scaled PageRank Update Rule}
First apply the Basic PageRank Update Rule.\\ 
\smallskip
Then scale down all PageRank values by a factor of s, shrinking the total from 1 to s. \\
\smallskip
We divide the residual $1 - s$ units of PageRank equally over all nodes, giving$ (1 - s)$/$n$ to each.
\end{block}
\end{frame}

\begin{frame}
\frametitle{Results}
We are going to present the result of the experiment, comparing the \alert{execution time} of PageRank on following inputs:
\begin{itemize}
\item Graph of 1000 nodes
\item Graph of 2000 nodes
\item Graph of 5000 nodes
\item Graph of 10000 nodes
\item Graph of 20000 nodes
\item Full Graph (30000 nodes) 
\end{itemize}
All the graphs are generated by chunking the Full Graph.
\end{frame}

\begin{frame}
\frametitle{Graph of Times}
\includegraphics[scale=0.5]{img/Ranking/Pagerank.PNG} 
\end{frame}

\subsection{HITS}
\begin{frame}
\frametitle{HITS}
This hubs-and-authorities algorithm, sometimes called HITS (\textit{hyperlink induced
topic search}), was originally intended not as a preprocessing step before
handling search queries, as PageRank is, but as a step to be done along with
the processing of a search query, to rank only the responses to that query.\\
\medskip
This kind of approach is  used by the Ask search engine.
\end{frame}

\begin{frame}
\frametitle{The Intuition Behind HITS}
HITS views important pages as having two different type of importance.
\begin{itemize}
\item Certain pages are valuable because they provide information about a
topic. These pages are called \textbf{authorities}.
\item Other pages are valuable not because they provide information about any
topic, but because they tell you where to go to find out about that topic.
These pages are called \textbf{hubs}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{HITS Algorithm}
For calculating the HITS values for the pages, we shall assign two scores to each Web page.
One score represents the \textit{hubbiness} of a page, that is the degree to which it
is a good hub, and the second score represents the degree to which the page
is a good authority.\\
\medskip
These values  are then calculated as:
\begin{itemize}
\item \onslide<1-> \textbf{Hubbiness}: the sum of the Authority value of the outgoing nodes.
\item \onslide<2-> \textbf{Authority}: the sum of Hubbines value of the incoming nodes.
\end{itemize}
\-\\\
\smallskip
\onslide<3-> These values are normalized so that the largest value is 1.
\end{frame}

\begin{frame}
\frametitle{Results}
We are going to present the result of the experiment, comparing the \alert{execution time} of HITS on following inputs:
\begin{itemize}
\item Graph of 1000 nodes
\item Graph of 2000 nodes
\item Graph of 5000 nodes
\item Graph of 10000 nodes
\item Graph of 20000 nodes
\item Full Graph (30000 nodes) 
\end{itemize}
All the graphs are generated by chunking the Full Graph.
\end{frame}

\begin{frame}
\frametitle{Graph of Times}
\includegraphics[scale=0.5]{img/Ranking/HITS.PNG} 
\end{frame}

\begin{frame}
\frametitle{Tuning for improving performance}
On the first attempts of running the algorithm on the full graph we observed that one iteration takes about 30 minutes, due to the nature of the algorithm. \\
In each iteration we explore all the graph and calculate the incoming nodes for the current node \dots\\
\bigskip
Considering that the graph never changes, we precomputed all the incoming nodes for each node so we can obtain the incoming nodes in $O(1)$.
\end{frame}

\begin{frame}
\frametitle{Tuning for improving performance}
In the HITS algorithm there are two stop rules:
\begin{itemize}
\item Max number of iteration
\item Min confidence on the errors reached
\end{itemize}
\medskip
We noticed that in some cases the Algorithm runs until the maximum number of steps is reached, and when this happens the relative error trends to stabilize on a fixed level. We decided then to add a new stop rule: \\
\medskip
\begin{itemize}
\item If two successive relative errors are distant at maximum a small $\epsilon$, then stop the algorithm.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parallel Version}
We also implemented the parallel version, but the performance resulted to be worst compared with the non-parallel version: this is due to overhead of coping all the structures needed for the calculation of scores.\\
\medskip
We now present times of 1 iteration of both version:
\begin{itemize}
\item \textbf{Non-Parallel}: 
\item \textbf{Parallel}:
\end{itemize} 
\end{frame}

\subsection{Comparing the Results}
\begin{frame}
\frametitle{Comparing the Results}
On the time side, the results are incomparable since they differ by $3$ orders of magnitude, even after the tuning.
CONTINUA CON I VALORI.
\end{frame}

\section{Matching}
\subsection{Best Match}
\begin{frame}
\frametitle{The idea of Best Match}
Given a query $q$, containing $n$ query words, and a set of documents $S$, we define Best Match as a method that finds a subset of document $S_{1}$ such that:
\begin{itemize}
	\item $S_{1}$ is included in $S$
	\item each document of $S_{1}$ has a "reasonable" number of query words in it
\end{itemize}
\medskip
According to this definition the basic Best Match consists of:
\begin{itemize}
	\item counting how many query words documents have
	\begin{itemize}
		\item we call this value "score" of a document
		\item it is at maximum $n$
	\end{itemize}
	\item ordering in decreasing order of score the documents (optional)
	\item return all documents whose score is "reasonable"
	\begin{itemize}
		\item we use a threshold to define what is "reasonable"
	\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Refining the Best Match}
Two are the basic refinements to have a more efficient Best Match:
\begin{itemize}
	\item using an inverted index
	\begin{itemize}
		\item in the form (word -> list of documents containing the word)
		\item we cicle only on query words instead of on all $S$
		\item we can have in O(1) all the documents with a determined word  
	\end{itemize}
	\item using the frequency instead of assigning score 1 to each query word found
	\begin{itemize}
		\item defined as number of occurrences in document $d$ / length($d$)
		\item requires precalculation of occurences for all words and all $S$...
		\item ...but it well represents the \textbf{relevance} of documents to a word or query
	\end{itemize}	
\end{itemize}
\end{frame}

\subsection{Improved Best Match}
\newcounter{sauvegardeenumi}
\newcommand{\asuivre}{\setcounter{sauvegardeenumi}{\theenumi}}
\newcommand{\suite}{\setcounter{enumi}{\thesauvegardeenumi}}

\begin{frame}
\frametitle{Improved Best Match}
We implemented also the following improved version of Best Match:
\begin{enumerate}
	\item Sort documents in each inverted index in order of frequency of the term at which the inverted index refers
	\item For every query term define its possible impact on the score as the frequency of the most frequent document in its index
	\item Sort the query terms in decreasing order of impact
	\item Consider the first 20 documents in the index of the first query term (if the first query term has an index with less than 20 documents, then complete with the first documents in the index of the next query term)
	\asuivre
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Improved Best Match}
\begin{enumerate}	
	\suite
	\item Compute the score for each of these documents
	\item Consider the first term in which there are documents that have not been scored
	\item Consider the first non-scored document in the index of this term
	\item If the frequency of the current term in the current document plus the sum of the impact of next terms is larger than the score of the 20-th scored document, then score this document and repeat from 7, otherwise consider the next    
\end{enumerate}
\end{frame}

\subsection{Results}
\begin{frame}
\frametitle{Creating the Dataset}
Our experiments ran on a set of approximatively 30000 pages created this way:
\begin{itemize}
	\item we choose a web-page for each of the 15 categories listed in https://www.dmoz.org/
	\item for every of these web pages we crawled 2000 pages by using the Wibbi online crawler
	\item Moreover, from each pair of sets of 2000 pages, we choose at random 10 pairs of vertices (u, v) with u being a page in the first set and v being a page in the second set and added a link from u to v (if this link was absent)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Creating the Dataset}
Here is the complete list of the websites chosen.
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{lll}
			\textbf{Category} & \textbf{Website}       & \textbf{Description}                                                                                                                                                                                         \\
			Arts              & www.imdb.com           & The Internet Movie Database. Features plot summaries, reviews, cast lists, and theatre schedules.                                                                                                            \\
			Business          & www.moodys.com         & Corporate finance, banking, managed funds and risk management.                                                                                                                                               \\
			Computers         & www.ibm.com            & Website for International Business Machines Corporation.                                                                                                                                                     \\
			Games             & www.ign.com            & Videogame news, previews and behind the scenes information.                                                                                                                                                  \\
			Health            & www.who.int            & World Health Organization. Health and health-related statistical information.                                                                                                                                \\
			Home              & www.cooks.com          & Recipe search, diet and nutrition, cooking articles and community forum.                                                                                                                                     \\
			Kids              & www.cartoonnetwork.com & The home of cartoons online - the best cartoons from the past and best new cartoons, on TV and on the web.                                                                                                                                  
		\end{tabular}%
	}
\end{table}
\end{frame}

\begin{frame}
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{lll}
			\textbf{Category} & \textbf{Website}       & \textbf{Description}                                                                                                                                                                                         \\
			News              & www.foxnews.com        & Breaking News, Latest News and Current News from FoxNews.com. Breaking news and video.                     \\
			Recreation        & www.lego.com           & Producer of building blocks.                                                                                                                                                                                 \\
			Reference         & www.britannica.com     & Encyclopaedia Britannica Online. Provides selected articles for free, full text with search capabilities, multimedia and related links for subscribers.                                                      \\
			Science           & www.nasa.gov           & Comprehensive, world-class center for aeronautics, atmospheric sciences and space technology.                   \\
			Shopping          & www.amazon.com         & Where customers can find and discover anything they might want to buy online, and endeavors to offer its customers the lowest possible prices. \\
			Society           & www.un.org             & Daily United Nations news, documents and publications, overview information, conference information, photos, and other UN information resources.                                                             \\
			Sports            & www.nba.com            & The official site of the National Basketball Association. Includes news, features, multimedia, player profiles, chat transcripts, schedules and statistics.                                                  \\
			Regional          & www.lonelyplanet.com   & Offers travel advice, detailed maps, travel news, popular message boards and health information. Also lists information and updates regarding guidebooks.                                                   
		\end{tabular}%
	}
\end{table}
\end{frame}

\begin{frame}
	\frametitle{Creating the Queries}
	
\end{frame}

\section{Search Engine}
\begin{frame}

\end{frame}

\section{Auction}

\subsection{First Price Auction}
\begin{frame}
\frametitle{First Price Auction}
In this kind of auction, bidders submit simultaneous ``sealed bids'' to the seller. The terminology comes from the original format for such auctions, in which bids were written down and provided in sealed envelopes to the seller, who would then open them all together.\\
\medskip
\begin{center}\textbf{The highest bidder wins the object \\ and pays the value of her bid.}\end{center}
\end{frame}

\begin{frame}
\frametitle{Non - truthfulness of FPA}
In a sealed-bid first-price auction, the value of your bid not only affects whether you win but also how much you pay.\\
\medskip
Bidding your true value is not a dominant strategy. By bidding your true value, you would get a payoff of 0 if you lose (as usual), and you would also get a payff of 0 if you win, since you’d pay exactly what it was worth to you.\\
\medskip
As a result, the optimal way to bid in a first-price auction is to “shade” your bid slightly downward, so that if you win you will get a positive payoff. Determining how much to shade your bid involves balancing a trade-off between two opposing forces.
\end{frame}

\subsection{Generalized Second Price Auction}
\begin{frame}
\frametitle{Generalized Second Price Auction}
Also called Vickrey auctions. Bidders submit simultaneous sealed bids to the sellers;
\begin{center} 
\textbf{The highest bidder wins the object \\
and pays the value of the second-highest bid.} 
\end{center}
These auctions are called Vickrey auctions in honor of William Vickrey, who wrote the first game-theoretic analysis of auctions. Vickery won the Nobel Memorial Prize in Economics in 1996 for this body of work.
\end{frame}

\begin{frame}
\frametitle{Truthfulness of GSP}
Truthful bidding is a dominant strategy in a sealed-bid second-price auction. The heart of the argument is the fact noted at the outset: in a second-price auction, your bid determines whether you win or lose, but not how much you pay in the event that you win. \\
\medskip
So in a second-price auction, it makes sense to bid your true value even if other bidders are overbidding, underbidding, colluding, or behaving in other unpredictable ways.
\end{frame}

\subsection{Results}
\begin{frame}
\frametitle{FPA}
\begin{center}
\includegraphics[scale=0.46]{img/Auctions/FPA_all.PNG} 
\end{center}
\end{frame}

\begin{frame}
\frametitle{GSP}
\begin{center}
\includegraphics[scale=0.46]{img/Auctions/GSP_all.PNG} 
\end{center}
\end{frame}



\end{document}